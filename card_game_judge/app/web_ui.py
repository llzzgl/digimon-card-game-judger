# æŠ‘åˆ¶è­¦å‘Š
import warnings
import os
import logging
import sys

os.environ["TOKENIZERS_PARALLELISM"] = "false"
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", message=".*torch.classes.*")
warnings.filterwarnings("ignore", message=".*Tried to instantiate class.*")

# æŠ‘åˆ¶ streamlit çš„ torch è­¦å‘Šæ—¥å¿—
logging.getLogger("streamlit.runtime.scriptrunner_utils.script_run_context").setLevel(logging.ERROR)
logging.getLogger("streamlit.watcher.path_watcher").setLevel(logging.ERROR)

import streamlit as st

# set_page_config å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤
st.set_page_config(
    page_title="å¡ç‰Œæ¸¸æˆæ™ºèƒ½è£åˆ¤",
    page_icon="ğŸ´",
    layout="wide"
)

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ä½¿ç”¨ st.cache_resource ç¼“å­˜é‡é‡çº§èµ„æºï¼Œé¿å…é‡å¤åŠ è½½
@st.cache_resource
def get_vector_store():
    from app.vector_store import vector_store
    # é¢„çƒ­ embedding æ¨¡å‹
    _ = vector_store.embeddings
    return vector_store

@st.cache_resource
def get_llm_service():
    from app.llm_service import llm_service
    return llm_service

@st.cache_resource
def get_query_processor():
    from app.query_processor import query_processor
    return query_processor

# æ˜¾ç¤ºåŠ è½½çŠ¶æ€
with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œé¦–æ¬¡å¯åŠ¨å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ..."):
    vector_store = get_vector_store()
    llm_service = get_llm_service()
    query_processor = get_query_processor()

from app.pdf_processor import extract_text_from_bytes
from app.models import DocumentType, DocumentMetadata

st.title("ğŸ´ å¡ç‰Œæ¸¸æˆæ™ºèƒ½è£åˆ¤")
st.caption("ä¸Šä¼ è§„åˆ™æ–‡æ¡£ï¼Œç„¶åå‘ AI è£åˆ¤æé—®")

# ä¾§è¾¹æ  - æ–‡æ¡£ç®¡ç†
with st.sidebar:
    st.header("ğŸ“š æ–‡æ¡£ç®¡ç†")
    
    # ä¸Šä¼ æ–‡ä»¶
    st.subheader("ä¸Šä¼ æ–‡ä»¶")
    uploaded_file = st.file_uploader("é€‰æ‹© PDFã€TXT æˆ– JSON æ–‡ä»¶", type=["pdf", "txt", "json"])
    file_doc_type = st.selectbox("æ–‡æ¡£ç±»å‹", ["rule", "ruling", "case"], 
                                  format_func=lambda x: {"rule": "ğŸ“˜ è§„åˆ™", "ruling": "âš–ï¸ è£å®š", "case": "ğŸ“‹ åˆ¤ä¾‹"}[x],
                                  key="file_type")
    file_title = st.text_input("æ–‡æ¡£æ ‡é¢˜", placeholder="ä¾‹å¦‚ï¼šæ¸¸æˆç‹è§„åˆ™æ‰‹å†Œ v1.0ï¼ˆJSONå¯ç•™ç©ºè‡ªåŠ¨ç”Ÿæˆï¼‰", key="file_title")
    file_tags = st.text_input("æ ‡ç­¾ï¼ˆé€—å·åˆ†éš”ï¼‰", placeholder="ä¾‹å¦‚ï¼šåŸºç¡€è§„åˆ™,æˆ˜æ–—", key="file_tags")
    
    if st.button("ğŸ“¤ ä¸Šä¼ æ–‡ä»¶", type="primary", use_container_width=True):
        if uploaded_file is None:
            st.error("è¯·é€‰æ‹©æ–‡ä»¶")
        else:
            try:
                content = uploaded_file.read()
                filename = uploaded_file.name
                
                # JSON æ–‡ä»¶ç‰¹æ®Šå¤„ç†ï¼ˆå¡ç‰Œæ•°æ®ï¼‰
                if filename.endswith('.json'):
                    import json
                    import time
                    json_data = json.loads(content.decode('utf-8'))
                    
                    # åˆ¤æ–­æ˜¯å¡ç‰Œæ•°ç»„è¿˜æ˜¯å•ä¸ªå¯¹è±¡
                    cards = json_data if isinstance(json_data, list) else [json_data]
                    total = len(cards)
                    
                    # åå°æ—¥å¿—
                    print(f"\n{'='*50}")
                    print(f"ğŸ“¤ å¼€å§‹å¯¼å…¥ JSON æ–‡ä»¶: {filename}")
                    print(f"   å…± {total} æ¡å¡ç‰Œæ•°æ®")
                    print(f"{'='*50}")
                    
                    # UI è¿›åº¦æ¡
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    success_count = 0
                    start_time = time.time()
                    
                    for i, card in enumerate(cards):
                        # æå–å¡ç‰Œä¿¡æ¯ç”Ÿæˆæ–‡æœ¬
                        card_text_parts = []
                        card_name = card.get('name', card.get('card_name', ''))
                        card_no = card.get('card_no', card.get('number', ''))
                        
                        for key, value in card.items():
                            if value and str(value).strip():
                                card_text_parts.append(f"{key}: {value}")
                        
                        card_text = "\n".join(card_text_parts)
                        if not card_text.strip():
                            print(f"   âš ï¸ [{i+1}/{total}] è·³è¿‡ç©ºå¡ç‰Œ")
                            continue
                        
                        title = file_title.strip() if file_title.strip() else f"{card_no} {card_name}".strip()
                        metadata = DocumentMetadata(
                            doc_type=DocumentType(file_doc_type),
                            title=title,
                            tags=[t.strip() for t in file_tags.split(",") if t.strip()] + ([card_no] if card_no else [])
                        )
                        result = vector_store.add_document(card_text, metadata)
                        success_count += 1
                        
                        # æ›´æ–°è¿›åº¦
                        progress = (i + 1) / total
                        progress_bar.progress(progress)
                        status_text.text(f"å¯¼å…¥ä¸­... {i+1}/{total} - {title}")
                        
                        # åå°æ—¥å¿—ï¼ˆæ¯10æ¡æˆ–æœ€åä¸€æ¡ï¼‰
                        if (i + 1) % 10 == 0 or i == total - 1:
                            elapsed = time.time() - start_time
                            print(f"   âœ“ [{i+1}/{total}] {title} ({result['chunk_count']} chunks) - {elapsed:.1f}s")
                    
                    elapsed = time.time() - start_time
                    progress_bar.empty()
                    status_text.empty()
                    
                    print(f"{'='*50}")
                    print(f"âœ… å¯¼å…¥å®Œæˆï¼æˆåŠŸ {success_count}/{total}ï¼Œè€—æ—¶ {elapsed:.1f}s")
                    print(f"{'='*50}\n")
                    
                    st.success(f"JSON å¯¼å…¥æˆåŠŸï¼å…±å¯¼å…¥ {success_count} æ¡å¡ç‰Œæ•°æ®ï¼Œè€—æ—¶ {elapsed:.1f}s")
                else:
                    # PDF/TXT å¤„ç†
                    if not file_title.strip():
                        st.error("è¯·è¾“å…¥æ–‡æ¡£æ ‡é¢˜")
                    else:
                        print(f"\nğŸ“¤ å¼€å§‹ä¸Šä¼ æ–‡ä»¶: {filename}")
                        
                        text = extract_text_from_bytes(content, filename)
                        if not text.strip():
                            st.error("æ— æ³•ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬")
                            print(f"   âŒ æ— æ³•æå–æ–‡æœ¬")
                        else:
                            metadata = DocumentMetadata(
                                doc_type=DocumentType(file_doc_type),
                                title=file_title.strip(),
                                tags=[t.strip() for t in file_tags.split(",") if t.strip()]
                            )
                            result = vector_store.add_document(text, metadata)
                            
                            print(f"   âœ… ä¸Šä¼ æˆåŠŸ: {file_title.strip()}")
                            print(f"      æ–‡æ¡£ID: {result['doc_id']}, åˆ†å—æ•°: {result['chunk_count']}\n")
                            
                            st.success(f"ä¸Šä¼ æˆåŠŸï¼æ–‡æ¡£ID: {result['doc_id']}, åˆ†å—æ•°: {result['chunk_count']}")
            except Exception as e:
                import traceback
                print(f"   âŒ ä¸Šä¼ å¤±è´¥: {str(e)}")
                st.error(f"ä¸Šä¼ å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
    
    st.divider()
    
    # æ·»åŠ æ–‡æœ¬
    st.subheader("æ·»åŠ æ–‡æœ¬")
    text_content = st.text_area("å†…å®¹", placeholder="ç›´æ¥ç²˜è´´è£å®šæˆ–åˆ¤ä¾‹å†…å®¹...", height=150)
    text_doc_type = st.selectbox("ç±»å‹", ["ruling", "case", "rule"],
                                  format_func=lambda x: {"rule": "ğŸ“˜ è§„åˆ™", "ruling": "âš–ï¸ è£å®š", "case": "ğŸ“‹ åˆ¤ä¾‹"}[x],
                                  key="text_type")
    text_title = st.text_input("æ ‡é¢˜", placeholder="ä¾‹å¦‚ï¼šå…³äºXXXæ•ˆæœçš„å®˜æ–¹è£å®š", key="text_title")
    text_tags = st.text_input("æ ‡ç­¾", placeholder="ä¾‹å¦‚ï¼šæ•ˆæœ,è¿é”", key="text_tags")
    
    if st.button("â• æ·»åŠ æ–‡æœ¬", use_container_width=True):
        if not text_content.strip():
            st.error("è¯·è¾“å…¥å†…å®¹")
        elif not text_title.strip():
            st.error("è¯·è¾“å…¥æ ‡é¢˜")
        else:
            try:
                metadata = DocumentMetadata(
                    doc_type=DocumentType(text_doc_type),
                    title=text_title.strip(),
                    tags=[t.strip() for t in text_tags.split(",") if t.strip()]
                )
                result = vector_store.add_document(text_content.strip(), metadata)
                st.success(f"æ·»åŠ æˆåŠŸï¼æ–‡æ¡£ID: {result['doc_id']}")
            except Exception as e:
                st.error(f"æ·»åŠ å¤±è´¥: {str(e)}")

# ä¸»åŒºåŸŸ - é—®ç­”
tab1, tab2, tab3 = st.tabs(["ğŸ’¬ æé—®", "ğŸ“š æ–‡æ¡£åˆ—è¡¨", "ğŸ”¬ Embedding æµ‹è¯•"])

with tab1:
    col1, col2 = st.columns([2, 1])
    
    with col1:
        question = st.text_area("ä½ çš„é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šå½“ä¸¤å¼ å¡ç‰ŒåŒæ—¶å‘åŠ¨æ•ˆæœæ—¶ï¼Œå¦‚ä½•åˆ¤æ–­ä¼˜å…ˆçº§ï¼Ÿ", height=100)
    
    with col2:
        doc_types = st.multiselect("æœç´¢èŒƒå›´ï¼ˆç•™ç©ºæœç´¢å…¨éƒ¨ï¼‰", ["rule", "ruling", "case"],
                                    format_func=lambda x: {"rule": "è§„åˆ™", "ruling": "è£å®š", "case": "åˆ¤ä¾‹"}[x])
        top_k = st.slider("å‚è€ƒæ–‡æ¡£æ•°é‡", 1, 10, 5)
    
    if st.button("ğŸ” æé—®", type="primary"):
        if not question.strip():
            st.warning("è¯·è¾“å…¥é—®é¢˜")
        else:
            # åˆ›å»ºæ—¥å¿—å®¹å™¨
            log_container = st.empty()
            logs = []
            
            def log_callback(msg: str):
                logs.append(f"{msg}")
                log_container.info("\n".join(logs))
            
            try:
                import time
                start_time = time.time()
                
                all_docs = []
                seen_contents = set()  # ç”¨äºå»é‡
                selected_types = [DocumentType(t) for t in doc_types] if doc_types else None
                
                # æ­¥éª¤0: æå–å¡ç‰Œç¼–å·å¹¶ç²¾ç¡®æ£€ç´¢
                card_numbers = query_processor.extract_card_numbers(question.strip())
                if card_numbers:
                    log_callback(f"ğŸ´ æ­¥éª¤0/4: å‘ç°å¡ç‰Œç¼–å·: {card_numbers}")
                    for card_no in card_numbers:
                        card_docs = vector_store.search_by_card_number(card_no, translate_result=True)
                        log_callback(f"   â””â”€ {card_no}: æ‰¾åˆ° {len(card_docs)} æ¡ç»“æœ")
                        for doc in card_docs:
                            content_hash = hash(doc["content"][:100])
                            if content_hash not in seen_contents:
                                seen_contents.add(content_hash)
                                all_docs.append(doc)
                else:
                    log_callback("ğŸ” æ­¥éª¤0/4: æœªå‘ç°å¡ç‰Œç¼–å·ï¼Œè·³è¿‡ç²¾ç¡®æœç´¢")
                
                # æ­¥éª¤1: å‘é‡æœç´¢ï¼ˆè§„åˆ™ç›¸å…³ï¼‰
                log_callback("ğŸ” æ­¥éª¤1/4: å¼€å§‹å‘é‡æœç´¢...")
                rule_docs = vector_store.search(
                    query=question.strip(), 
                    doc_types=selected_types, 
                    top_k=top_k,
                    translate_result=True
                )
                for doc in rule_docs:
                    content_hash = hash(doc["content"][:100])
                    if content_hash not in seen_contents:
                        seen_contents.add(content_hash)
                        all_docs.append(doc)
                
                # é™åˆ¶æ€»æ•°ï¼Œä½†ç¡®ä¿å¡ç‰Œä¿¡æ¯ä¼˜å…ˆ
                max_docs = top_k + len(card_numbers) * 2
                docs = all_docs[:max_docs]
                
                log_callback(f"âœ… æœç´¢å®Œæˆï¼Œå…± {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆå¡ç‰Œ: {len(all_docs) - len(rule_docs)}ï¼Œè§„åˆ™: {len(rule_docs)}ï¼‰")
                
                if not docs:
                    log_container.empty()
                    st.warning("çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·å…ˆä¸Šä¼ è§„åˆ™æ–‡æ¡£ã€‚")
                else:
                    # æ­¥éª¤2-4: è°ƒç”¨ LLMï¼ˆå†…éƒ¨æœ‰è¯¦ç»†æ—¥å¿—ï¼‰
                    answer = llm_service.generate_answer(question.strip(), docs, log_callback=log_callback)
                    
                    elapsed = time.time() - start_time
                    log_callback(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼æ€»è€—æ—¶ {elapsed:.1f}s")
                    
                    # æ¸…é™¤æ—¥å¿—ï¼Œæ˜¾ç¤ºç»“æœ
                    log_container.empty()
                    
                    st.subheader("å›ç­”")
                    st.markdown(answer)
                    
                    st.subheader("å‚è€ƒæ¥æº")
                    # å…ˆæ˜¾ç¤ºå¡ç‰Œä¿¡æ¯ï¼Œå†æ˜¾ç¤ºè§„åˆ™
                    card_docs_shown = []
                    rule_docs_shown = []
                    for doc in docs:
                        if doc.get('score', 1) == 0.0:  # ç²¾ç¡®åŒ¹é…çš„å¡ç‰Œ
                            card_docs_shown.append(doc)
                        else:
                            rule_docs_shown.append(doc)
                    
                    if card_docs_shown:
                        st.markdown("**ğŸ´ å¡ç‰Œä¿¡æ¯**")
                        for doc in card_docs_shown:
                            with st.expander(f"ğŸ´ {doc['metadata'].get('title', 'æœªçŸ¥')}"):
                                st.write(doc['content'][:800] + "..." if len(doc['content']) > 800 else doc['content'])
                    
                    if rule_docs_shown:
                        st.markdown("**ğŸ“š è§„åˆ™å‚è€ƒ**")
                        for doc in rule_docs_shown:
                            doc_type_label = {"rule": "ğŸ“˜è§„åˆ™", "ruling": "âš–ï¸è£å®š", "case": "ğŸ“‹åˆ¤ä¾‹"}.get(doc['doc_type'], "ğŸ“„")
                            with st.expander(f"{doc_type_label} {doc['metadata'].get('title', 'æœªçŸ¥')}"):
                                st.write(doc['content'][:500] + "..." if len(doc['content']) > 500 else doc['content'])
                            
            except TimeoutError as e:
                log_container.empty()
                st.error(f"â° {str(e)}")
            except Exception as e:
                import traceback
                log_container.empty()
                st.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
                st.code(traceback.format_exc())

with tab2:
    if st.button("ğŸ”„ åˆ·æ–°åˆ—è¡¨"):
        st.rerun()
    
    try:
        docs = vector_store.list_documents()
        if not docs:
            st.info("ğŸ“­ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
        else:
            for doc in docs:
                doc_type_label = {"rule": "ğŸ“˜è§„åˆ™", "ruling": "âš–ï¸è£å®š", "case": "ğŸ“‹åˆ¤ä¾‹"}.get(doc.get("doc_type", ""), "ğŸ“„")
                
                with st.expander(f"{doc_type_label} {doc.get('title', 'æœªçŸ¥')} ({doc.get('chunk_count', 0)} ä¸ªåˆ†å—)"):
                    st.markdown(f"""
- **æ–‡æ¡£ID**: `{doc.get('doc_id', '')}`
- **ç±»å‹**: {doc.get('doc_type', '')}
- **æ ‡ç­¾**: {doc.get('tags', '') or 'æ— '}
- **åˆ›å»ºæ—¶é—´**: {doc.get('created_at', '')[:19] if doc.get('created_at') else 'æœªçŸ¥'}
""")
                    
                    # æŸ¥çœ‹åˆ†å—æŒ‰é’®
                    if st.button(f"ğŸ“„ æŸ¥çœ‹åˆ†å—å†…å®¹", key=f"view_{doc.get('doc_id')}"):
                        chunks = vector_store.get_document_chunks(doc.get('doc_id'))
                        if chunks:
                            for chunk in chunks:
                                st.markdown(f"**åˆ†å— {chunk['chunk_index'] + 1}**")
                                st.text_area(
                                    label=f"chunk_{chunk['chunk_index']}", 
                                    value=chunk['content'], 
                                    height=150,
                                    key=f"chunk_{doc.get('doc_id')}_{chunk['chunk_index']}",
                                    label_visibility="collapsed"
                                )
                        else:
                            st.warning("æœªæ‰¾åˆ°åˆ†å—å†…å®¹")
    except Exception as e:
        import traceback
        st.error(f"è·å–å¤±è´¥: {str(e)}")
        st.code(traceback.format_exc())

with tab3:
    st.subheader("ğŸ”¬ Embedding æµ‹è¯•")
    st.caption("æµ‹è¯•å‘é‡æœç´¢æ˜¯å¦æ­£å¸¸å·¥ä½œï¼Œä¸è°ƒç”¨ LLM")
    
    test_query = st.text_input("æµ‹è¯•æŸ¥è¯¢", placeholder="è¾“å…¥å…³é”®è¯æµ‹è¯•æœç´¢...")
    test_top_k = st.slider("è¿”å›ç»“æœæ•°", 1, 10, 3, key="test_top_k")
    
    if st.button("ğŸ” æµ‹è¯•æœç´¢", type="primary"):
        if not test_query.strip():
            st.warning("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹")
        else:
            with st.spinner("æœç´¢ä¸­..."):
                try:
                    docs = vector_store.search(query=test_query.strip(), top_k=test_top_k)
                    
                    if not docs:
                        st.warning("âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œå¯èƒ½åŸå› ï¼š\n1. çŸ¥è¯†åº“ä¸ºç©º\n2. æŸ¥è¯¢ä¸æ–‡æ¡£å†…å®¹ä¸ç›¸å…³\n3. Embedding æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
                    else:
                        st.success(f"âœ… æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³ç»“æœï¼ŒEmbedding å·¥ä½œæ­£å¸¸ï¼")
                        
                        for i, doc in enumerate(docs, 1):
                            score = doc.get('score', 0)
                            # åˆ†æ•°è¶Šä½è¶Šç›¸ä¼¼ï¼ˆL2è·ç¦»ï¼‰
                            similarity = "é«˜" if score < 0.5 else "ä¸­" if score < 1.0 else "ä½"
                            
                            with st.expander(f"ç»“æœ {i} - ç›¸ä¼¼åº¦: {similarity} (è·ç¦»: {score:.4f})"):
                                st.markdown(f"**æ¥æº**: {doc['metadata'].get('title', 'æœªçŸ¥')}")
                                st.markdown(f"**ç±»å‹**: {doc['doc_type']}")
                                st.markdown(f"**åˆ†å—ç´¢å¼•**: {doc['metadata'].get('chunk_index', '?')}")
                                st.divider()
                                st.markdown("**å†…å®¹**:")
                                st.text(doc['content'])
                                
                except Exception as e:
                    import traceback
                    st.error(f"æœç´¢å¤±è´¥: {str(e)}")
                    st.code(traceback.format_exc())
    
    st.divider()
    st.subheader("ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡")
    
    if st.button("æŸ¥çœ‹ç»Ÿè®¡"):
        try:
            docs = vector_store.list_documents()
            if docs:
                total_chunks = sum(d.get('chunk_count', 0) for d in docs)
                by_type = {}
                for d in docs:
                    t = d.get('doc_type', 'unknown')
                    by_type[t] = by_type.get(t, 0) + 1
                
                col1, col2, col3 = st.columns(3)
                col1.metric("æ–‡æ¡£æ€»æ•°", len(docs))
                col2.metric("åˆ†å—æ€»æ•°", total_chunks)
                col3.metric("å¹³å‡åˆ†å—/æ–‡æ¡£", f"{total_chunks/len(docs):.1f}" if docs else "0")
                
                st.markdown("**æŒ‰ç±»å‹ç»Ÿè®¡**:")
                for t, count in by_type.items():
                    label = {"rule": "ğŸ“˜è§„åˆ™", "ruling": "âš–ï¸è£å®š", "case": "ğŸ“‹åˆ¤ä¾‹"}.get(t, t)
                    st.write(f"- {label}: {count} ä¸ªæ–‡æ¡£")
            else:
                st.info("çŸ¥è¯†åº“ä¸ºç©º")
        except Exception as e:
            st.error(f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}")
