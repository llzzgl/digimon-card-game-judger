from langchain_openai import ChatOpenAI
from langchain_community.llms import Ollama
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from typing import List
import time
import os
import httpx

from app.config import LLM_MODEL, OPENAI_API_KEY, GOOGLE_API_KEY

# å¦‚æœéœ€è¦ä»£ç†è®¿é—® Google APIï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šå¹¶ä¿®æ”¹ä»£ç†åœ°å€
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"

# é…ç½®æ›´çŸ­çš„è¶…æ—¶æ—¶é—´
DEFAULT_TIMEOUT = httpx.Timeout(connect=10.0, read=60.0, write=10.0, pool=10.0)


SYSTEM_PROMPT = """ä½ æ˜¯æ•°ç å®è´å¡ç‰Œæ¸¸æˆï¼ˆDTCGï¼‰è£åˆ¤ã€‚

ã€å…³äºå¡ç‰Œæ•ˆæœ - ä¸¥æ ¼è¦æ±‚ã€‘
1. å¼•ç”¨å¡ç‰Œæ•ˆæœæ—¶ï¼Œå¿…é¡»ä»ã€å‚è€ƒèµ„æ–™ã€‘ä¸­åŸæ–‡å¤åˆ¶ï¼Œä¸€å­—ä¸æ”¹
2. ç»å¯¹ç¦æ­¢ç¼–é€ ã€ç¿»è¯‘ã€çŒœæµ‹æˆ–ä¿®æ”¹å¡ç‰Œæ•ˆæœæ–‡æœ¬
3. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰æŸå¼ å¡ç‰Œçš„æ•°æ®ï¼Œæ˜ç¡®è¯´"å‚è€ƒèµ„æ–™ä¸­æœªæä¾›è¯¥å¡ç‰Œæ•°æ®"
4. å¡ç‰Œæ•ˆæœå¿…é¡»ç”¨ä¸­æ–‡ï¼ˆå‚è€ƒèµ„æ–™æ˜¯ä¸­æ–‡çš„ï¼‰

ã€å…³äºè§„åˆ™è£å®š - ä½ å¯ä»¥åˆ†æã€‘
1. æ ¹æ®å‚è€ƒèµ„æ–™ä¸­çš„è§„åˆ™æ–‡æ¡£åˆ†ææ•ˆæœå¤„ç†é¡ºåº
2. åˆ¤æ–­æ•ˆæœçš„å‘åŠ¨æ—¶æœºå’Œæ¡ä»¶
3. è§£é‡Šè§„åˆ™çš„é€‚ç”¨æƒ…å†µ
4. ç»™å‡ºè£å®šç»“è®º

ã€å›ç­”æ ¼å¼ã€‘
1. å…ˆåˆ—å‡ºæ¶‰åŠçš„å¡ç‰Œæ•ˆæœï¼ˆç›´æ¥å¤åˆ¶å‚è€ƒèµ„æ–™åŸæ–‡ï¼‰
2. åˆ†ææ•ˆæœå‘åŠ¨æ—¶æœºå’Œå¤„ç†é¡ºåº
3. ç»™å‡ºè£å®šç»“è®º

ã€å‚è€ƒèµ„æ–™ã€‘
{context}
"""

USER_PROMPT = """ã€é—®é¢˜ã€‘
{question}

è¯·æ ¹æ®å‚è€ƒèµ„æ–™å›ç­”ã€‚å¼•ç”¨å¡ç‰Œæ•ˆæœæ—¶å¿…é¡»åŸæ–‡å¤åˆ¶ï¼Œä¸è¦æ”¹å†™æˆ–ç¿»è¯‘ã€‚"""


class LLMService:
    def __init__(self):
        self.llm = self._init_llm()
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", USER_PROMPT)
        ])
        self.timeout = 60  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    def _init_llm(self):
        if LLM_MODEL == "local":
            return Ollama(model="qwen2:7b", temperature=0)
        elif LLM_MODEL == "gemini":
            return ChatGoogleGenerativeAI(
                model="gemini-2.5-flash", 
                temperature=0,  # é™åˆ°0ï¼Œå‡å°‘ç¼–é€ 
                google_api_key=GOOGLE_API_KEY,
                timeout=60,
                max_retries=2
            )
        else:
            return ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0,
                openai_api_key=OPENAI_API_KEY,
                timeout=60,
                max_retries=2
            )
    
    def _call_llm(self, context: str, question: str) -> str:
        """å®é™…è°ƒç”¨ LLM çš„æ–¹æ³•"""
        chain = self.prompt | self.llm
        response = chain.invoke({"context": context, "question": question})
        if hasattr(response, 'content'):
            return response.content
        return str(response)
    
    def generate_answer(self, question: str, context_docs: List[dict], log_callback=None) -> str:
        """æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆå›ç­”ï¼Œå¸¦æ—¥å¿—"""
        def log(msg: str):
            if log_callback:
                log_callback(msg)
            print(f"[LLM] {msg}")
        
        start_time = time.time()
        
        # æ­¥éª¤1: æ„å»ºä¸Šä¸‹æ–‡
        log("ğŸ“ æ­¥éª¤1/3: æ„å»ºä¸Šä¸‹æ–‡...")
        context_parts = []
        for i, doc in enumerate(context_docs, 1):
            title = doc['metadata'].get('title', 'æœªçŸ¥æ¥æº')
            doc_type = doc.get('doc_type', '')
            type_label = {"rule": "è§„åˆ™", "ruling": "å®˜æ–¹è£å®š", "case": "åˆ¤ä¾‹"}.get(doc_type, "æ–‡æ¡£")
            
            # æå–å¡ç‰Œç¼–å·ï¼ˆå¦‚æœæœ‰ï¼‰
            content = doc['content']
            card_no = ""
            if "card_no:" in content.lower():
                import re
                match = re.search(r'card_no:\s*([A-Z0-9-_]+)', content, re.IGNORECASE)
                if match:
                    card_no = f" [{match.group(1)}]"
            
            context_parts.append(
                f"ã€å‚è€ƒ{i}ã€‘{card_no}\n"
                f"æ¥æºï¼š{title}ï¼ˆ{type_label}ï¼‰\n"
                f"å†…å®¹ï¼š{content}\n"
            )
        
        context = "\n\n".join(context_parts)
        log(f"âœ… ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼Œå…± {len(context_docs)} ä¸ªå‚è€ƒæ–‡æ¡£ï¼Œ{len(context)} å­—ç¬¦")
        
        # è°ƒè¯•ï¼šæ‰“å°å®é™…ä¼ ç»™ LLM çš„ä¸Šä¸‹æ–‡
        print("=" * 60)
        print("ã€è°ƒè¯•ã€‘ä¼ ç»™ LLM çš„å‚è€ƒèµ„æ–™å†…å®¹ï¼š")
        print("=" * 60)
        print(context[:2000])  # åªæ‰“å°å‰2000å­—ç¬¦
        if len(context) > 2000:
            print(f"... (å…± {len(context)} å­—ç¬¦)")
        print("=" * 60)
        
        # æ­¥éª¤2: è°ƒç”¨ LLM
        log(f"ğŸ¤– æ­¥éª¤2/3: è°ƒç”¨ LLM ({LLM_MODEL})...")
        
        try:
            result = self._call_llm(context, question)
            elapsed = time.time() - start_time
            log(f"âœ… LLM å“åº”å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}s")
            
            # æ­¥éª¤3: è¿”å›ç»“æœ
            log(f"ğŸ“¤ æ­¥éª¤3/3: è¿”å›ç»“æœï¼Œå…± {len(result)} å­—ç¬¦")
            return result
                    
        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            log(f"âŒ LLM è°ƒç”¨å¤±è´¥ï¼Œè€—æ—¶ {elapsed:.1f}s")
            log(f"âŒ é”™è¯¯è¯¦æƒ…: {error_msg}")
            
            # æ£€æŸ¥å¸¸è§é”™è¯¯
            if "API key" in error_msg.lower() or "invalid" in error_msg.lower():
                log("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ GOOGLE_API_KEY æ˜¯å¦æ­£ç¡®è®¾ç½®")
            elif "quota" in error_msg.lower():
                log("ğŸ’¡ æç¤º: API é…é¢å·²ç”¨å®Œï¼Œè¯·ç¨åé‡è¯•")
            elif "network" in error_msg.lower() or "connection" in error_msg.lower():
                log("ğŸ’¡ æç¤º: ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œå¯èƒ½éœ€è¦ä»£ç†")
            
            raise


llm_service = LLMService()
