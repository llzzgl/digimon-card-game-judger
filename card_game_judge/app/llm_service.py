from langchain_openai import ChatOpenAI
from langchain_community.llms import Ollama
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from typing import List
import time
import os

from app.config import LLM_MODEL, OPENAI_API_KEY, GOOGLE_API_KEY

# å¦‚æœéœ€è¦ä»£ç†è®¿é—® Google APIï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šå¹¶ä¿®æ”¹ä»£ç†åœ°å€
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"


SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°ç å®è´å¡ç‰Œæ¸¸æˆï¼ˆDTCGï¼‰è£åˆ¤ã€‚ä½ çš„èŒè´£æ˜¯æ ¹æ®ç©å®¶æè¿°çš„æ¸¸æˆåœºé¢ï¼Œç»“åˆè§„åˆ™å’Œå¡ç‰Œæ•ˆæœï¼Œç»™å‡ºå‡†ç¡®çš„è£å®šã€‚

ã€ä½ çš„å·¥ä½œæ–¹å¼ã€‘
1. ä»”ç»†åˆ†æç©å®¶æè¿°çš„åœºé¢çŠ¶å†µ
2. è¯†åˆ«æ¶‰åŠçš„å¡ç‰ŒåŠå…¶æ•ˆæœ
3. æ ¹æ®è§„åˆ™åˆ¤æ–­æ•ˆæœçš„å‘åŠ¨æ¡ä»¶å’Œå¤„ç†é¡ºåº
4. ç»™å‡ºæ¸…æ™°ã€æœ‰æ¡ç†çš„è£å®šè¯´æ˜

ã€å›ç­”æ ¼å¼è¦æ±‚ã€‘
1. å…ˆåˆ—å‡ºæ¶‰åŠçš„å¡ç‰ŒåŠå…¶å…³é”®æ•ˆæœ
2. åˆ†ææ•ˆæœçš„å‘åŠ¨æ—¶æœºå’Œæ¡ä»¶
3. æŒ‰ç…§æ­£ç¡®çš„å¤„ç†é¡ºåºè¯´æ˜æ¯ä¸€æ­¥
4. å¦‚æœ‰å¤šç§å¯èƒ½çš„å¤„ç†æ–¹å¼ï¼Œåˆ†åˆ«è¯´æ˜
5. å¼•ç”¨è§„åˆ™æ—¶æ ‡æ³¨æ¥æºï¼Œå¦‚ã€Œæ ¹æ®ã€å‚è€ƒ1ã€‘...ã€

ã€é‡è¦è§„åˆ™æé†’ã€‘
- æ•ˆæœå¤„ç†éµå¾ª"å…ˆå‘åŠ¨å…ˆå¤„ç†"åŸåˆ™
- åŒæ—¶æ»¡è¶³å‘åŠ¨æ¡ä»¶çš„æ•ˆæœï¼Œå›åˆç©å®¶ä¼˜å…ˆé€‰æ‹©å¤„ç†é¡ºåº
- ã€ç™»åœºæ—¶ã€‘ã€è¿›åŒ–æ—¶ã€‘ç­‰æ—¶æœºæ•ˆæœåœ¨å¯¹åº”åŠ¨ä½œå®Œæˆåå‘åŠ¨
- è¿é”æ•ˆæœéœ€è¦æŒ‰ç…§æ­£ç¡®é¡ºåºé€ä¸€å¤„ç†

ã€å‚è€ƒèµ„æ–™ã€‘
{context}
"""

USER_PROMPT = """ã€ç©å®¶æé—®ã€‘
{question}

è¯·ä½œä¸ºè£åˆ¤ï¼Œåˆ†æä¸Šè¿°åœºé¢å¹¶ç»™å‡ºè£å®šã€‚è¦æ±‚ï¼š
1. åˆ—å‡ºæ¶‰åŠçš„å¡ç‰Œæ•ˆæœ
2. è¯´æ˜æ•ˆæœå¤„ç†é¡ºåº
3. ç»™å‡ºæœ€ç»ˆè£å®šç»“æœ"""


class LLMService:
    def __init__(self):
        self.llm = self._init_llm()
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", USER_PROMPT)
        ])
        self.timeout = 60  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    def _init_llm(self):
        if LLM_MODEL == "local":
            return Ollama(model="qwen2:7b", temperature=0.1)
        elif LLM_MODEL == "gemini":
            return ChatGoogleGenerativeAI(
                model="gemini-2.5-flash",
                temperature=0.1,
                google_api_key=GOOGLE_API_KEY
            )
        else:
            return ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.1,
                openai_api_key=OPENAI_API_KEY
            )
    
    def _call_llm(self, context: str, question: str) -> str:
        """å®é™…è°ƒç”¨ LLM çš„æ–¹æ³•"""
        chain = self.prompt | self.llm
        response = chain.invoke({"context": context, "question": question})
        if hasattr(response, 'content'):
            return response.content
        return str(response)
    
    def generate_answer(self, question: str, context_docs: List[dict], log_callback=None) -> str:
        """æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆå›ç­”ï¼Œå¸¦æ—¥å¿—"""
        def log(msg: str):
            if log_callback:
                log_callback(msg)
            print(f"[LLM] {msg}")
        
        start_time = time.time()
        
        # æ­¥éª¤1: æ„å»ºä¸Šä¸‹æ–‡
        log("ğŸ“ æ­¥éª¤1/3: æ„å»ºä¸Šä¸‹æ–‡...")
        context_parts = []
        for i, doc in enumerate(context_docs, 1):
            title = doc['metadata'].get('title', 'æœªçŸ¥æ¥æº')
            doc_type = doc.get('doc_type', '')
            type_label = {"rule": "è§„åˆ™", "ruling": "å®˜æ–¹è£å®š", "case": "åˆ¤ä¾‹"}.get(doc_type, "æ–‡æ¡£")
            
            context_parts.append(
                f"ã€å‚è€ƒ{i}ã€‘\n"
                f"æ¥æºï¼š{title}ï¼ˆ{type_label}ï¼‰\n"
                f"å†…å®¹ï¼š{doc['content']}\n"
            )
        
        context = "\n\n".join(context_parts)
        log(f"âœ… ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼Œå…± {len(context_docs)} ä¸ªå‚è€ƒæ–‡æ¡£ï¼Œ{len(context)} å­—ç¬¦")
        
        # æ­¥éª¤2: è°ƒç”¨ LLM
        log(f"ğŸ¤– æ­¥éª¤2/3: è°ƒç”¨ LLM ({LLM_MODEL})...")
        
        try:
            result = self._call_llm(context, question)
            elapsed = time.time() - start_time
            log(f"âœ… LLM å“åº”å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}s")
            
            # æ­¥éª¤3: è¿”å›ç»“æœ
            log(f"ğŸ“¤ æ­¥éª¤3/3: è¿”å›ç»“æœï¼Œå…± {len(result)} å­—ç¬¦")
            return result
                    
        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            log(f"âŒ LLM è°ƒç”¨å¤±è´¥ï¼Œè€—æ—¶ {elapsed:.1f}s")
            log(f"âŒ é”™è¯¯è¯¦æƒ…: {error_msg}")
            
            # æ£€æŸ¥å¸¸è§é”™è¯¯
            if "API key" in error_msg.lower() or "invalid" in error_msg.lower():
                log("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ GOOGLE_API_KEY æ˜¯å¦æ­£ç¡®è®¾ç½®")
            elif "quota" in error_msg.lower():
                log("ğŸ’¡ æç¤º: API é…é¢å·²ç”¨å®Œï¼Œè¯·ç¨åé‡è¯•")
            elif "network" in error_msg.lower() or "connection" in error_msg.lower():
                log("ğŸ’¡ æç¤º: ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œå¯èƒ½éœ€è¦ä»£ç†")
            
            raise


llm_service = LLMService()
