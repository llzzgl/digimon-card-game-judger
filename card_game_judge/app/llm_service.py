from langchain_openai import ChatOpenAI
from langchain_community.llms import Ollama
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from typing import List
import time
import os
import httpx

from app.config import LLM_MODEL, OPENAI_API_KEY, GOOGLE_API_KEY

# é€šä¹‰åƒé—® API Key
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY", "")

# å¦‚æœéœ€è¦ä»£ç†è®¿é—® Google API
if os.getenv("USE_PROXY", "").lower() == "true":
    os.environ["HTTP_PROXY"] = f"http://{os.getenv('PROXY_HOST', '127.0.0.1')}:{os.getenv('PROXY_PORT', '7897')}"
    os.environ["HTTPS_PROXY"] = f"http://{os.getenv('PROXY_HOST', '127.0.0.1')}:{os.getenv('PROXY_PORT', '7897')}"


SYSTEM_PROMPT = """ä½ æ˜¯æ•°ç å®è´å¡ç‰Œæ¸¸æˆï¼ˆDTCGï¼‰è£åˆ¤åŠ©æ‰‹ã€‚

ã€é‡è¦æé†’ã€‘
- å¡ç‰Œæ•ˆæœå·²åœ¨ç•Œé¢ä¸Šå•ç‹¬æ˜¾ç¤ºï¼Œä½ ä¸éœ€è¦åˆ—å‡ºå¡ç‰Œæ•ˆæœ
- ä½ çš„åˆ†æä»…ä¾›å‚è€ƒï¼Œå¤æ‚æƒ…å†µè¯·ä»¥å®˜æ–¹è£å®šä¸ºå‡†
- å¦‚æœè§„åˆ™å‚è€ƒä¸­æ²¡æœ‰ç›´æ¥ç›¸å…³çš„è§„åˆ™ï¼Œè¯·æ˜ç¡®è¯´æ˜"è§„åˆ™å‚è€ƒä¸­æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³æ¡æ¬¾"

ã€ä½ çš„ä»»åŠ¡ã€‘
æ ¹æ®ä¸‹æ–¹ã€è§„åˆ™å‚è€ƒã€‘åˆ†æç©å®¶çš„é—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨ï¼š
1. æ•ˆæœçš„è§¦å‘æ—¶æœºï¼ˆä»€ä¹ˆæ—¶å€™è§¦å‘ï¼‰
2. æ•ˆæœçš„å¤„ç†é¡ºåºï¼ˆå…ˆå¤„ç†ä»€ä¹ˆï¼Œåå¤„ç†ä»€ä¹ˆï¼‰
3. ç»™å‡ºè£å®šå»ºè®®

ã€è§„åˆ™å‚è€ƒã€‘
{context}

å¦‚æœè§„åˆ™å‚è€ƒä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯šå®è¯´æ˜ã€‚
"""

USER_PROMPT = """ã€é—®é¢˜ã€‘
{question}

è¯·æ ¹æ®è§„åˆ™å‚è€ƒåˆ†æã€‚å¦‚æœè§„åˆ™å‚è€ƒä¸è¶³ï¼Œè¯·è¯´æ˜ã€‚"""


class LLMService:
    def __init__(self):
        self.llm = self._init_llm()
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", USER_PROMPT)
        ])
        self.timeout = 60  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    def _init_llm(self):
        if LLM_MODEL == "local":
            return Ollama(model="qwen2:7b", temperature=0)
        elif LLM_MODEL == "gemini":
            return ChatGoogleGenerativeAI(
                model="gemini-2.5-flash", 
                temperature=0,
                google_api_key=GOOGLE_API_KEY,
                timeout=60,
                max_retries=2
            )
        elif LLM_MODEL == "qwen":
            # é€šä¹‰åƒé—® - ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
            return ChatOpenAI(
                model="qwen-flash",  # å¯é€‰: qwen-turbo, qwen-plus, qwen-max
                temperature=0,
                openai_api_key=DASHSCOPE_API_KEY,
                openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
                timeout=60,
                max_retries=2
            )
        else:
            # OpenAI
            return ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0,
                openai_api_key=OPENAI_API_KEY,
                timeout=60,
                max_retries=2
            )
    
    def _call_llm(self, context: str, question: str) -> str:
        """å®é™…è°ƒç”¨ LLM çš„æ–¹æ³•"""
        chain = self.prompt | self.llm
        response = chain.invoke({"context": context, "question": question})
        if hasattr(response, 'content'):
            return response.content
        return str(response)
    
    def generate_answer(self, question: str, context_docs: List[dict], log_callback=None) -> str:
        """æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆå›ç­”ï¼Œå¸¦æ—¥å¿—"""
        def log(msg: str):
            if log_callback:
                log_callback(msg)
            print(f"[LLM] {msg}")
        
        start_time = time.time()
        
        # æ­¥éª¤1: æ„å»ºä¸Šä¸‹æ–‡
        log("ğŸ“ æ­¥éª¤1/3: æ„å»ºä¸Šä¸‹æ–‡...")
        context_parts = []
        for i, doc in enumerate(context_docs, 1):
            title = doc['metadata'].get('title', 'æœªçŸ¥æ¥æº')
            doc_type = doc.get('doc_type', '')
            type_label = {"rule": "è§„åˆ™", "ruling": "å®˜æ–¹è£å®š", "case": "åˆ¤ä¾‹"}.get(doc_type, "æ–‡æ¡£")
            
            # æå–å¡ç‰Œç¼–å·ï¼ˆå¦‚æœæœ‰ï¼‰
            content = doc['content']
            card_no = ""
            if "card_no:" in content.lower():
                import re
                match = re.search(r'card_no:\s*([A-Z0-9-_]+)', content, re.IGNORECASE)
                if match:
                    card_no = f" [{match.group(1)}]"
            
            context_parts.append(
                f"ã€å‚è€ƒ{i}ã€‘{card_no}\n"
                f"æ¥æºï¼š{title}ï¼ˆ{type_label}ï¼‰\n"
                f"å†…å®¹ï¼š{content}\n"
            )
        
        context = "\n\n".join(context_parts)
        log(f"âœ… ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼Œå…± {len(context_docs)} ä¸ªå‚è€ƒæ–‡æ¡£ï¼Œ{len(context)} å­—ç¬¦")
        
        # è°ƒè¯•ï¼šæ‰“å°å®é™…ä¼ ç»™ LLM çš„ä¸Šä¸‹æ–‡
        print("=" * 60)
        print("ã€è°ƒè¯•ã€‘ä¼ ç»™ LLM çš„å‚è€ƒèµ„æ–™å†…å®¹ï¼š")
        print("=" * 60)
        print(context[:2000])  # åªæ‰“å°å‰2000å­—ç¬¦
        if len(context) > 2000:
            print(f"... (å…± {len(context)} å­—ç¬¦)")
        print("=" * 60)
        
        # æ­¥éª¤2: è°ƒç”¨ LLM
        log(f"ğŸ¤– æ­¥éª¤2/3: è°ƒç”¨ LLM ({LLM_MODEL})...")
        
        try:
            result = self._call_llm(context, question)
            elapsed = time.time() - start_time
            log(f"âœ… LLM å“åº”å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}s")
            
            # æ­¥éª¤3: è¿”å›ç»“æœ
            log(f"ğŸ“¤ æ­¥éª¤3/3: è¿”å›ç»“æœï¼Œå…± {len(result)} å­—ç¬¦")
            return result
                    
        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            log(f"âŒ LLM è°ƒç”¨å¤±è´¥ï¼Œè€—æ—¶ {elapsed:.1f}s")
            log(f"âŒ é”™è¯¯è¯¦æƒ…: {error_msg}")
            
            # æ£€æŸ¥å¸¸è§é”™è¯¯
            if "API key" in error_msg.lower() or "invalid" in error_msg.lower():
                log("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ GOOGLE_API_KEY æ˜¯å¦æ­£ç¡®è®¾ç½®")
            elif "quota" in error_msg.lower():
                log("ğŸ’¡ æç¤º: API é…é¢å·²ç”¨å®Œï¼Œè¯·ç¨åé‡è¯•")
            elif "network" in error_msg.lower() or "connection" in error_msg.lower():
                log("ğŸ’¡ æç¤º: ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œå¯èƒ½éœ€è¦ä»£ç†")
            
            raise


llm_service = LLMService()
