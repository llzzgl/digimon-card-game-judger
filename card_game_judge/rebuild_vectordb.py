"""
é‡å»ºå‘é‡æ•°æ®åº“ - ä½¿ç”¨æ–°çš„å¤šè¯­è¨€ embedding æ¨¡å‹
è¿è¡Œå‰ä¼šæ¸…ç©ºç°æœ‰æ•°æ®ï¼

ç”¨æ³•:
  python rebuild_vectordb.py                    # é‡å»ºå…¨éƒ¨æ•°æ®
  python rebuild_vectordb.py --import-rules     # å¯¼å…¥è§„åˆ™ä¹¦ï¼ˆå¼¹å‡ºæ–‡ä»¶é€‰æ‹©æ¡†ï¼‰
  python rebuild_vectordb.py --import-rules path/to/file.pdf  # å¯¼å…¥æŒ‡å®šè§„åˆ™ä¹¦
"""
import os
import shutil
import time
import argparse

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import warnings
warnings.filterwarnings("ignore")

from tqdm import tqdm
from pathlib import Path


def import_rule_files(file_paths=None):
    """
    å¯¼å…¥è§„åˆ™ä¹¦æ–‡ä»¶
    
    Args:
        file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœä¸º None åˆ™å¼¹å‡ºæ–‡ä»¶é€‰æ‹©æ¡†
    """
    # å¯¼å…¥æ¨¡å—
    import sys
    sys.path.insert(0, '.')
    from app.vector_store import vector_store
    from app.pdf_processor import extract_text_from_bytes
    from app.models import DocumentType, DocumentMetadata
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶ï¼Œå¼¹å‡ºæ–‡ä»¶é€‰æ‹©æ¡†
    if not file_paths:
        try:
            import tkinter as tk
            from tkinter import filedialog
            
            root = tk.Tk()
            root.withdraw()  # éšè—ä¸»çª—å£
            root.attributes('-topmost', True)  # ç½®é¡¶
            
            file_paths = filedialog.askopenfilenames(
                title="é€‰æ‹©è§„åˆ™ä¹¦æ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰",
                filetypes=[
                    ("æ”¯æŒçš„æ–‡ä»¶", "*.pdf *.txt *.json"),
                    ("PDF æ–‡ä»¶", "*.pdf"),
                    ("æ–‡æœ¬æ–‡ä»¶", "*.txt"),
                    ("JSON æ–‡ä»¶", "*.json"),
                    ("æ‰€æœ‰æ–‡ä»¶", "*.*")
                ]
            )
            root.destroy()
            
            if not file_paths:
                print("æœªé€‰æ‹©æ–‡ä»¶ï¼Œé€€å‡º")
                return
        except Exception as e:
            print(f"æ— æ³•æ‰“å¼€æ–‡ä»¶é€‰æ‹©æ¡†: {e}")
            print("è¯·ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šæ–‡ä»¶è·¯å¾„: python rebuild_vectordb.py --import-rules path/to/file.pdf")
            return
    
    # ç¡®ä¿æ˜¯åˆ—è¡¨
    if isinstance(file_paths, str):
        file_paths = [file_paths]
    
    print(f"\nå‡†å¤‡å¯¼å…¥ {len(file_paths)} ä¸ªè§„åˆ™ä¹¦æ–‡ä»¶...")
    print("=" * 50)
    
    success = 0
    failed = 0
    total_chunks = 0
    
    for file_path in file_paths:
        p = Path(file_path)
        if not p.exists():
            print(f"  âœ— æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            failed += 1
            continue
        
        try:
            print(f"\nğŸ“„ å¤„ç†: {p.name}")
            
            content = p.read_bytes()
            text = extract_text_from_bytes(content, p.name)
            
            if not text.strip():
                print(f"  âœ— æ— æ³•æå–æ–‡æœ¬")
                failed += 1
                continue
            
            # ç”Ÿæˆæ ‡é¢˜ï¼ˆå»æ‰æ‰©å±•åï¼‰
            title = p.stem
            
            metadata = DocumentMetadata(
                doc_type=DocumentType.RULE,
                title=title,
                source=str(p.absolute()),
                tags=['è§„åˆ™ä¹¦', 'å®˜æ–¹è§„åˆ™']
            )
            
            result = vector_store.add_document(text, metadata)
            total_chunks += result['chunk_count']
            success += 1
            
            print(f"  âœ“ å¯¼å…¥æˆåŠŸ: {result['chunk_count']} chunks")
            
        except Exception as e:
            print(f"  âœ— å¯¼å…¥å¤±è´¥: {e}")
            failed += 1
    
    print("\n" + "=" * 50)
    print(f"è§„åˆ™ä¹¦å¯¼å…¥å®Œæˆ: æˆåŠŸ {success}, å¤±è´¥ {failed}, æ€»è®¡ {total_chunks} chunks")


def rebuild_all():
    """é‡å»ºå…¨éƒ¨å‘é‡æ•°æ®åº“"""
    # æ¸…ç©ºç°æœ‰å‘é‡åº“
    CHROMA_DIR = os.path.join(os.path.dirname(__file__), "data", "chroma_db")
    if os.path.exists(CHROMA_DIR):
        print(f"æ¸…ç©ºç°æœ‰å‘é‡åº“: {CHROMA_DIR}")
        try:
            shutil.rmtree(CHROMA_DIR)
        except PermissionError:
            print("æ–‡ä»¶è¢«å ç”¨ï¼Œå°è¯•å¼ºåˆ¶åˆ é™¤...")
            import gc
            gc.collect()
            time.sleep(1)
            for root, dirs, files in os.walk(CHROMA_DIR, topdown=False):
                for name in files:
                    try:
                        os.remove(os.path.join(root, name))
                    except:
                        pass
                for name in dirs:
                    try:
                        os.rmdir(os.path.join(root, name))
                    except:
                        pass
            try:
                os.rmdir(CHROMA_DIR)
            except:
                print("æ— æ³•å®Œå…¨æ¸…ç©ºï¼Œå°†ä½¿ç”¨æ–°ç›®å½•")
                CHROMA_DIR = CHROMA_DIR + "_new"
        os.makedirs(CHROMA_DIR, exist_ok=True)
        print("å·²æ¸…ç©º")

    print("\nå¼€å§‹é‡æ–°å¯¼å…¥æ•°æ®...")
    print("=" * 50)

    # å¯¼å…¥æ¨¡å—
    import sys
    sys.path.insert(0, '.')
    from app.vector_store import vector_store
    from app.pdf_processor import extract_text_from_bytes
    from app.models import DocumentType, DocumentMetadata

    # 1. å¯¼å…¥æœ¯è¯­å¯¹ç…§è¡¨
    print("\n[1/3] å¯¼å…¥æœ¯è¯­å¯¹ç…§è¡¨...")
    terminology_files = [
        ('../digimon_data/dtcg_terminology.json', 'DTCGæœ¯è¯­å¯¹ç…§è¡¨'),
        ('../digimon_data/digimon_name_mapping.json', 'DTCGæ•°ç å®è´åç§°å¯¹ç…§è¡¨')
    ]

    for file_path, title in tqdm(terminology_files, desc="æœ¯è¯­å¯¹ç…§è¡¨", unit="file"):
        p = Path(file_path)
        if not p.exists():
            continue
        content = p.read_bytes()
        text = extract_text_from_bytes(content, p.name)
        
        metadata = DocumentMetadata(
            doc_type=DocumentType.RULE,
            title=title,
            source=str(p),
            tags=['æœ¯è¯­', 'ç¿»è¯‘', 'æ—¥ä¸­å¯¹ç…§']
        )
        
        result = vector_store.add_document(text, metadata)

    print("  æœ¯è¯­å¯¹ç…§è¡¨å¯¼å…¥å®Œæˆ")

    # 2. å¯¼å…¥è§„åˆ™ä¹¦ PDF
    print("\n[2/3] å¯¼å…¥è§„åˆ™ä¹¦...")
    rule_files = [
        ('æ•°ç å®è´å¡ç‰Œå¯¹æˆ˜ ç»¼åˆè§„åˆ™ 2025.12 æ—¥æ–‡ç‰ˆ.pdf', 'ç»¼åˆè§„åˆ™ 2025.12 æ—¥æ–‡ç‰ˆ'),
        ('æ•°ç å®è´å¡ç‰Œå¯¹æˆ˜ ç»¼åˆè§„åˆ™1.2ï¼ˆ2024-02-16ï¼‰.pdf', 'ç»¼åˆè§„åˆ™ 1.2 ä¸­æ–‡ç‰ˆ'),
        ('æ•°ç å®è´å¡ç‰Œå¯¹æˆ˜_ç»¼åˆè§„åˆ™_æœ€æ–°ç‰ˆ_ä¸­æ–‡ç¿»è¯‘_gemini.txt', 'ç»¼åˆè§„åˆ™ æœ€æ–°ç‰ˆ ä¸­æ–‡ç¿»è¯‘'),
    ]

    for file_name, title in tqdm(rule_files, desc="è§„åˆ™ä¹¦", unit="file"):
        p = Path(file_name)
        if not p.exists():
            continue
        
        try:
            content = p.read_bytes()
            text = extract_text_from_bytes(content, p.name)
            
            if not text.strip():
                continue
            
            metadata = DocumentMetadata(
                doc_type=DocumentType.RULE,
                title=title,
                source=str(p),
                tags=['è§„åˆ™ä¹¦', 'å®˜æ–¹è§„åˆ™']
            )
            
            result = vector_store.add_document(text, metadata)
            tqdm.write(f"  âœ“ {title}: {result['chunk_count']} chunks")
        except Exception as e:
            tqdm.write(f"  âœ— {title}: {e}")

    print("  è§„åˆ™ä¹¦å¯¼å…¥å®Œæˆ")

    # 3. å¯¼å…¥å¡ç‰Œæ•°æ®
    print("\n[3/3] å¯¼å…¥å¡ç‰Œæ•°æ®...")
    card_data_dir = Path('../digimon_card_data')
    if card_data_dir.exists():
        files = list(card_data_dir.glob('*.json'))
        print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶\n")
        
        success = 0
        failed = 0
        total_chunks = 0
        
        with tqdm(files, desc="å¡ç‰Œæ•°æ®", unit="file", ncols=80) as pbar:
            for file_path in pbar:
                try:
                    title = file_path.stem
                    if title.startswith('digimon_cards_'):
                        title = title[len('digimon_cards_'):]
                    
                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    short_title = title[:20] + "..." if len(title) > 20 else title
                    pbar.set_postfix_str(short_title)
                    
                    content = file_path.read_bytes()
                    text = extract_text_from_bytes(content, file_path.name)
                    
                    if not text.strip():
                        continue
                    
                    metadata = DocumentMetadata(
                        doc_type=DocumentType.RULE,
                        title=title,
                        source=str(file_path),
                        tags=['dtcgå¡ç‰Œæ•°æ®åº“']
                    )
                    
                    result = vector_store.add_document(text, metadata)
                    total_chunks += result['chunk_count']
                    success += 1
                except Exception as e:
                    failed += 1
                    tqdm.write(f"  âœ— {file_path.name}: {e}")
        
        print(f"\nå¡ç‰Œæ•°æ®å¯¼å…¥å®Œæˆ: æˆåŠŸ {success}, å¤±è´¥ {failed}, æ€»è®¡ {total_chunks} chunks")
    else:
        print("  å¡ç‰Œæ•°æ®ç›®å½•ä¸å­˜åœ¨")

    print("\n" + "=" * 50)
    print("é‡å»ºå®Œæˆï¼")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å‘é‡æ•°æ®åº“ç®¡ç†å·¥å…·")
    parser.add_argument("--import-rules", nargs='*', metavar="FILE",
                        help="å¯¼å…¥è§„åˆ™ä¹¦æ–‡ä»¶ï¼ˆä¸æŒ‡å®šæ–‡ä»¶åˆ™å¼¹å‡ºé€‰æ‹©æ¡†ï¼‰")
    
    args = parser.parse_args()
    
    if args.import_rules is not None:
        # å¯¼å…¥è§„åˆ™ä¹¦æ¨¡å¼
        import_rule_files(args.import_rules if args.import_rules else None)
    else:
        # é‡å»ºå…¨éƒ¨
        rebuild_all()
