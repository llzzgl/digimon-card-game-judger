# å¿«é€Ÿå¼€å§‹ - DTCG å¾®è°ƒ

## ğŸ“ å‡†å¤‡å·¥ä½œ

### ç¡®è®¤æºæ•°æ®æ–‡ä»¶

ç¡®ä¿ `origin_data/` æ–‡ä»¶å¤¹ä¸­åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

```
origin_data/
â”œâ”€â”€ rulebook.txt        # è§„åˆ™ä¹¦æ–‡ä»¶
â”œâ”€â”€ cards.json          # å¡ç‰Œæ•°æ®æ–‡ä»¶
â””â”€â”€ official_qa.json    # å®˜æ–¹ Q&A æ–‡ä»¶
```

å¦‚æœç¼ºå°‘æ–‡ä»¶ï¼Œè¯·å‚è€ƒ [origin_data/README.md](origin_data/README.md) äº†è§£å¦‚ä½•å‡†å¤‡æ•°æ®ã€‚

---

## ä¸€é”®æ”¶é›†æ•°æ®å¹¶å¼€å§‹å¾®è°ƒ

### æ­¥éª¤ 1ï¼šæ”¶é›†è®­ç»ƒæ•°æ®

```bash
cd card_game_judge/finetune
python collect_all_data.py
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
============================================================
DTCG å¾®è°ƒæ•°æ®å®Œæ•´æ”¶é›†
============================================================

ã€æ­¥éª¤ 1ã€‘ä»è§„åˆ™ä¹¦æå–é—®ç­”...
âœ… ä»è§„åˆ™ä¹¦æå–äº† XXX æ¡é—®ç­”

ã€æ­¥éª¤ 2ã€‘åŠ è½½å®˜æ–¹ Q&A...
âœ… åŠ è½½äº† XXX æ¡å®˜æ–¹ Q&A

ã€æ­¥éª¤ 3ã€‘åŠ è½½å¡ç‰Œæ•°æ®...
âœ… ä»å¡ç‰Œæ•°æ®ç”Ÿæˆäº† 17,656 æ¡é—®ç­”

ğŸ“Š æ€»è®¡ç”Ÿæˆ 17,658 æ¡è®­ç»ƒæ•°æ®
   â€¢ è§„åˆ™ä¹¦é—®ç­”: 0
   â€¢ å®˜æ–¹ Q&A: 2
   â€¢ å¡ç‰Œæ•°æ®é—®ç­”: 17,656
   â€¢ è‡ªå®šä¹‰é—®ç­”: 0

ğŸ“ è¾“å‡ºæ–‡ä»¶:
   â€¢ training_data\dtcg_finetune_data.jsonl
   â€¢ training_data\dtcg_finetune_data.json
   â€¢ training_data\dtcg_conversation.jsonl
```

### æ­¥éª¤ 2ï¼šå¼€å§‹å¾®è°ƒ

```bash
# ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆQwen2-1.5Bï¼‰
python finetune_qwen.py --data training_data/dtcg_finetune_data.jsonl

# æˆ–ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆæ¨èï¼‰
python finetune_qwen.py \
    --model Qwen/Qwen2-7B-Instruct \
    --data training_data/dtcg_finetune_data.jsonl \
    --epochs 3 \
    --batch_size 2 \
    --output output/dtcg_qwen_7b_lora
```

### æ­¥éª¤ 3ï¼šç­‰å¾…è®­ç»ƒå®Œæˆ

è®­ç»ƒæ—¶é—´å–å†³äºï¼š
- æ¨¡å‹å¤§å°ï¼ˆ1.5B vs 7Bï¼‰
- GPU æ€§èƒ½
- æ•°æ®é‡ï¼ˆ17,658 æ¡ï¼‰
- è®­ç»ƒè½®æ•°

**é¢„è®¡æ—¶é—´ï¼š**
- Qwen2-1.5B + RTX 3090: ~2-3 å°æ—¶
- Qwen2-7B + RTX 3090: ~6-8 å°æ—¶

### æ­¥éª¤ 4ï¼šæµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹

```python
from finetune_qwen import DTCGFineTuner

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
model, tokenizer = DTCGFineTuner.load_finetuned(
    lora_path="output/dtcg_qwen_lora",
    base_model="Qwen/Qwen2-1.5B-Instruct"
)

# æµ‹è¯•æŸ¥è¯¢
prompt = "EX11-026 æ˜¯ä»€ä¹ˆå¡ï¼Ÿ"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_length=512)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## å¸¸è§é—®é¢˜

### Q1: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**æ–¹æ¡ˆ 1ï¼šå‡å° batch size**
```bash
python finetune_qwen.py \
    --data training_data/dtcg_finetune_data.jsonl \
    --batch_size 1 \
    --gradient_accumulation_steps 16
```

**æ–¹æ¡ˆ 2ï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹**
```bash
python finetune_qwen.py \
    --model Qwen/Qwen2-0.5B-Instruct \
    --data training_data/dtcg_finetune_data.jsonl
```

**æ–¹æ¡ˆ 3ï¼šå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆé»˜è®¤å·²å¯ç”¨ï¼‰**
```bash
python finetune_qwen.py \
    --data training_data/dtcg_finetune_data.jsonl \
    --use_4bit  # é»˜è®¤å¯ç”¨
```

### Q2: å¦‚ä½•åªä½¿ç”¨å¡ç‰Œæ•°æ®è®­ç»ƒï¼Ÿ

```bash
# æ”¶é›†æ•°æ®æ—¶è·³è¿‡è§„åˆ™ä¹¦
python collect_all_data.py --no-cards

# æˆ–æ‰‹åŠ¨ç¼–è¾‘ data_collector.py
```

### Q3: å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰é—®ç­”ï¼Ÿ

```python
from data_collector import DTCGDataCollector

collector = DTCGDataCollector()

# æ·»åŠ è‡ªå®šä¹‰é—®ç­”
collector.add_custom_qa(
    question="ä»€ä¹ˆæ˜¯æ•°ç åˆä½“ï¼Ÿ",
    answer="æ•°ç åˆä½“æ˜¯ä¸€ç§ç‰¹æ®Šçš„ç™»åœºæ–¹å¼...",
    tags=["è§„åˆ™", "æ•°ç åˆä½“"]
)

# å¯¼å‡º
collector.export_jsonl("custom_data.jsonl")
```

### Q4: è®­ç»ƒä¸­æ–­äº†æ€ä¹ˆåŠï¼Ÿ

å¾®è°ƒè„šæœ¬ä¼šè‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥ä»æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒï¼š

```bash
python finetune_qwen.py \
    --data training_data/dtcg_finetune_data.jsonl \
    --output output/dtcg_qwen_lora  # ä½¿ç”¨ç›¸åŒçš„è¾“å‡ºç›®å½•
```

### Q5: å¦‚ä½•åˆå¹¶ LoRA æƒé‡ï¼Ÿ

```bash
python finetune_qwen.py \
    --data training_data/dtcg_finetune_data.jsonl \
    --merge  # è®­ç»ƒå®Œæˆåè‡ªåŠ¨åˆå¹¶
```

æˆ–ä½¿ç”¨ Pythonï¼š

```python
from finetune_qwen import DTCGFineTuner, FinetuneConfig

config = FinetuneConfig(output_dir="output/dtcg_qwen_lora")
trainer = DTCGFineTuner(config)
trainer.load_model()
trainer.merge_and_save("output/dtcg_qwen_merged")
```

## é«˜çº§é…ç½®

### è°ƒæ•´ LoRA å‚æ•°

```bash
python finetune_qwen.py \
    --data training_data/dtcg_finetune_data.jsonl \
    --lora_r 128 \
    --lora_alpha 256 \
    --lora_dropout 0.1
```

### è°ƒæ•´å­¦ä¹ ç‡å’Œè®­ç»ƒè½®æ•°

```bash
python finetune_qwen.py \
    --data training_data/dtcg_finetune_data.jsonl \
    --epochs 5 \
    --learning_rate 1e-4
```

### ä½¿ç”¨å¤š GPU è®­ç»ƒ

```bash
# ä½¿ç”¨ torchrun
torchrun --nproc_per_node=2 finetune_qwen.py \
    --data training_data/dtcg_finetune_data.jsonl
```

## ç›‘æ§è®­ç»ƒè¿›åº¦

è®­ç»ƒæ—¥å¿—ä¼šä¿å­˜åœ¨ `output/dtcg_qwen_lora/runs/`ï¼Œå¯ä»¥ä½¿ç”¨ TensorBoard æŸ¥çœ‹ï¼š

```bash
tensorboard --logdir output/dtcg_qwen_lora/runs
```

## ä¸‹ä¸€æ­¥

- æŸ¥çœ‹ [README_CARD_DATA.md](README_CARD_DATA.md) äº†è§£å¡ç‰Œæ•°æ®é›†æˆè¯¦æƒ…
- æŸ¥çœ‹ [finetune_qwen.py](finetune_qwen.py) äº†è§£å¾®è°ƒè„šæœ¬è¯¦æƒ…
- æŸ¥çœ‹ [data_collector.py](data_collector.py) äº†è§£æ•°æ®æ”¶é›†è¯¦æƒ…
